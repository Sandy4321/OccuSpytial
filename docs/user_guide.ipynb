{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input format\n",
    "==========\n",
    "To demonstrate the python data types required to represent the input, we use the utility function ``make_data`` which returns randomly generated data that we will use for this example. See the function's documentation for more information regarding its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from occuspytial.utils import make_data\n",
    "\n",
    "Q, W, X, y, true_alpha, true_beta, true_tau, true_z = make_data(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gibbs samplers expect (at the minimum) as input the design matrix for occupancy covariates, the design matrices for detection covariates, the detection/non-detection data and the spatial precision matrix of the ICAR spatial random effects.\n",
    "\n",
    "``Q`` represents the spatial precision matrix and can either be scipy's sparse matrix object or a numpy array. If a numpy array is used, the sampler convert it to a sparse matrix. It is advised that ``Q`` be in sparse format since the format is more memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection/non-detection observed data (``y``) should be depresented using a dictionary whose key is the site number (only sites that were surveyed) and value is a numpy array whose elements are 1's (if the species is detected on that visit) and 0's. The length of the array should represent the number of visits for that site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[15]  # 4 visits in site 15 with no detection of the species on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the detection covariates (``W``) are represented by a dictionary whose key is the site number (only sites that were surveyed) and value is a 2d numpy array representing the design matrix of the detection covariates of that site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occupancy covariates (``X``) are represented by a design matrix which is a 2d numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]  # occupancy design matrix for the first 5 sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the output from ``make_data`` are the simulated true values for spatial occupancy model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('alpha: ', true_alpha, '\\n', 'beta: ', true_beta, '\\n', 'tau: ', true_tau, '\\n', 'z (occupancy state): ', true_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling example\n",
    "===============\n",
    "\n",
    "This section contains basic examples of how to use the available samplers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gibbs sampling\n",
    "----------------------\n",
    "Here we show how to use the Gibbs sampler presented in [Clark & Altwegg (2019)](https://onlinelibrary.wiley.com/doi/full/10.1002/ece3.4850) using OccuSpytial's sampler API. The name of the class implementing this sampler is ``LogitRSRGibbs``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from occuspytial import LogitRSRGibbs\n",
    "from occuspytial.utils import make_data\n",
    "\n",
    "# generate fake data with 500 sites in total\n",
    "Q, W, X, y, true_alpha, true_beta, true_tau, true_z = make_data(500, tau_range=(0.1, 0.5), random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print out the true values of the parameters of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('alpha: ', true_alpha, '\\n', 'beta: ', true_beta, '\\n', 'tau: ', true_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsr = LogitRSRGibbs(Q, W, X, y, random_state=10)\n",
    "rsr_samples = rsr.sample(2000, burnin=1000, chains=2)\n",
    "# The progressbar is on by default, but Jupyter notebook only displays it on the console\n",
    "# so it is not visible in the output cell of this notebook unlike if we are working on the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the sample method is an instance of the ``PosteriorParameter`` class and inference of the samples obtained are done via the instance stored in the variable ``rsr_samples``. We can display the summary table using the `summary` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsr_samples.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the individual parameters we can use the the parameter's name as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsr_samples['alpha']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we generated 2 chains, ``alpha`` parameter array is three dimensional; where the first dimension is the chain index,\n",
    "the second dimension is the length of the post-burnin samples, and the third dimension is the size of the parameter (in elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsr_samples['alpha'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``LogitRSRGibbs`` expects the prior distributions for ``alpha`` and ``beta`` to be normal distributed, and the prior\n",
    "for ``tau`` to be Gamma distributed. We can pass custom values for the hyperparameters of these priors through the `hparams` dictionary parameter during instantiation of the class instance. More details on legal keys and values can be found in the docstring of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_size = true_alpha.shape[0]\n",
    "b_size = true_beta.shape[0]\n",
    "hypers = {\n",
    "    'a_mu': np.ones(a_size),  # alpha mean is an array of 1's\n",
    "    'a_prec': np.eye(a_size) / 1000,  # alpha precision matrix (inverse of covariance) is diagonal matrix with entries (1/1000)\n",
    "    'b_mu': np.ones(b_size),\n",
    "    'b_prec': np.eye(a_size) / 1000,\n",
    "    'tau_rate': 2,  # tau's rate parameter for the prior gamma distribution\n",
    "    'tau_shape': 2,  # tau's shape parameter\n",
    "}\n",
    "rsr_hp = LogitRSRGibbs(Q, W, X, y, hparams=hypers, random_state=10)\n",
    "rsrhp_samples = rsr_hp.sample(2000, burnin=1000, chains=2)\n",
    "rsrhp_samples.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the MCMC chain did not converge in just 2000 samples with the provided hyper-parameter values. We can either\n",
    "improve the estimates by using a much longer chain or use better starting values in the ``sample`` method via the `start` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing posterior samples\n",
    "========================\n",
    "\n",
    "``rsr_samples`` instance allows for convenient visualization of posterior samples of the parameters of interest. This functionality uses\n",
    "[arviz](https://arviz-devs.github.io/arviz/index.html) as a backend. We can plot the traces and densities as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsr_samples.plot_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters can be passed into the ``plot_trace`` method to configure the output plot. See arviz's [API reference](https://arviz-devs.github.io/arviz/generated/arviz.plot_trace.html#arviz.plot_trace) for valid input. Similarly, the Effective Sample Size (ESS) can be visualized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsr_samples.plot_ess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about other available plots, see documentation of ``PosteriorParameter`` class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
